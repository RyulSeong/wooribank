{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4fdf0fb2",
   "metadata": {},
   "source": [
    "## 전처리이후 - Actor 도출"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263e141f",
   "metadata": {},
   "source": [
    "- 토큰화 형태소\n",
    "- 불용어사전\n",
    "- 임베딩\n",
    "- Ward clustering 군집 덴드로그램 \n",
    "- 실루엣계수 조정\n",
    "- 빈도분석\n",
    "- TF - IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26916c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install keras_tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709fa0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61ef0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install kss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9c055d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "from pandas import Series\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "import re\n",
    "\n",
    "from konlpy.tag import Okt\n",
    "# from konlpy.tag import Mecab \n",
    "\n",
    "#pip install kss\n",
    "from kss import split_sentences   \n",
    "#from pykospacing import spacing\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from tensorflow.keras.layers import Embedding, Dense,GRU,Flatten, LSTM,Conv1D, GlobalMaxPooling1D, Embedding, Dropout, GlobalAveragePooling1D\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from kerastuner.tuners import RandomSearch # 랜덤서치를 합니다\n",
    "\n",
    "from gensim .models import Word2Vec\n",
    "from sklearn.manifold import TSNE\n",
    "from matplotlib import font_manager as fm\n",
    "from matplotlib import rc\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fcdba8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c5981a6c",
   "metadata": {},
   "source": [
    "### 전처리 파일 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984d1a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"우리_크롤링_전처리후.csv\", encoding='utf-8-sig')\n",
    "\n",
    "del data['Unnamed: 0']\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880db959",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.dropna().reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804ac38b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0f5108ee",
   "metadata": {},
   "source": [
    "### 토큰화 형태소/불용어 사전 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5536fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_okt(text):\n",
    "    #     text = spacing(text) # 띄어쓰기 보정 위에서 했으면 필요없습니다\n",
    "    pos_words = okt.pos(text, stem=True)\n",
    "    words = [word for word, tag in pos_words if tag in ['Noun', 'Adjective', 'Verb', 'KoreanParticle', 'VerbPrefix'] ]\n",
    "    stopwords = ['하다', '되다', '안', '기', '고', '요', '란', '다음', '요즘', '지금', '앞', '왜', '여기', '후', '다른', '함', '등', '동안', '원래', '아주', '날', '더', '진짜', '이', '월', '시간', '오늘', '저', '또', '그', '좀', '년', '린지', '정말', '블로그', '그냥', '사실', '이제', '때문', '이번', '제', '다시', '정도', '시', '못', '주간', '일기', '하루', '일상', '전', '난', '일', '걸', '뭐', '줄', '만', '건', '분', '개', '끝', '잼', '이거', '번', '중', '듯', '때', '게', '내', '말', '나', '수', '거', '점', '것']\n",
    "    stopped_words = [w for w in words if not w in stopwords]\n",
    "    return stopped_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b589237",
   "metadata": {},
   "outputs": [],
   "source": [
    "okt = Okt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed668a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(range(len(data))):\n",
    "    data['review'].iloc[i] = preprocess_okt(data['review'].iloc[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4668d93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['review']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa086c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "73144858",
   "metadata": {},
   "source": [
    "### 임베딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de73c780",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 20 # 임베딩 크기는 논문을 따름\n",
    "model_shopping = Word2Vec(sentences=data.review, sg=1, vector_size=EMBEDDING_DIM, window=5, min_count=1) #sg 0은 CBOW, 1은 SKIP-GRAM\n",
    "w2v_vocab_shopping = list(model_shopping.wv.key_to_index) # 임베딩 된 단어 리스트\n",
    "print('Vocabulary size : ',len(w2v_vocab_shopping)) \n",
    "print('Vecotr shape :',model_shopping.wv.vectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285e897b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"shopping :\" ,model_shopping.wv.most_similar('대학생')) #TEST용\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eec657d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model in ASCII (word2vec) format\n",
    "# 텍스트 파일로 단어들의 임베딩 벡터 저장\n",
    "filename = 'shopping_word2vec.txt'\n",
    "model_shopping.wv.save_word2vec_format(filename, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9313ac0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "430e4175",
   "metadata": {},
   "source": [
    "### Ward clustering 군집 덴드로그램 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2582a195",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.metrics.cluster import silhouette_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0750b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ward linkage (가장 일반적으로 사용됨)\n",
    "\n",
    "def visualize_silhouette_layer1(data, num_cluster):\n",
    "    clusters_range = range(2,int(num_cluster))\n",
    "    results = []\n",
    "\n",
    "    for i in clusters_range:\n",
    "        clusterer = AgglomerativeClustering(n_clusters=i,linkage='ward')\n",
    "        cluster_labels = clusterer.fit_predict(data)\n",
    "        silhouette_avg = silhouette_score(data, cluster_labels)\n",
    "        results.append([i, silhouette_avg])\n",
    "\n",
    "    result = pd.DataFrame(results, columns=[\"n_clusters\", \"silhouette_score\"])\n",
    "    pivot_ac = pd.pivot_table(result, index=\"n_clusters\", values=\"silhouette_score\")\n",
    "\n",
    "    return result, pivot_ac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939d18ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_shopping.wv[\"있다\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc29ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "rv = []\n",
    "for i in data[\"review\"]:\n",
    "    review_vector = 0\n",
    "    for w in i:\n",
    "        review_vector += model_shopping.wv[w]\n",
    "    try:\n",
    "        rv.append(review_vector/len(i))\n",
    "    except:\n",
    "        rv.append(0)\n",
    "data[\"review vector\"] = rv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f73c739",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"review vector\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a429401",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 추가된 코드\n",
    "useless = []\n",
    "for a, i in enumerate(data[\"review vector\"]):\n",
    "    if type(i) == int:\n",
    "        useless.append(i)\n",
    "for i in useless:\n",
    "    data = data.drop(data.index[i])\n",
    "data = data.reset_index(drop = True)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd79fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(data[\"review vector\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013aa996",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"review vector\"] = np.array(data[\"review vector\"],dtype=object)\n",
    "print(data[\"review vector\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0044161",
   "metadata": {},
   "outputs": [],
   "source": [
    "rv = []\n",
    "for i in data[\"review vector\"]:\n",
    "    if type(i) != int:\n",
    "        rv.append(i)\n",
    "len(rv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb67b9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "linked = linkage(rv, 'ward')\n",
    "\n",
    "plt.figure(figsize=(15, 9))\n",
    "dendrogram(linked,\n",
    "            orientation='top',\n",
    "            distance_sort='descending',\n",
    "            show_leaf_counts=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf83c8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "07f1471f",
   "metadata": {},
   "source": [
    "### 실루엣 계수 조정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475b6a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_silhouette_layer1(data[\"review vector\"], num_cluster):\n",
    "    clusters_range = range(2,int(num_cluster))\n",
    "    results = []\n",
    "\n",
    "    for i in clusters_range:\n",
    "        clusterer = AgglomerativeClustering(n_clusters=i,linkage='ward')\n",
    "        cluster_labels = clusterer.fit_predict(data['review vector'])\n",
    "        silhouette_avg = silhouette_score(data['review vector'], cluster_labels)\n",
    "        results.append([i, silhouette_avg])\n",
    "\n",
    "    result = pd.DataFrame(results, columns=[\"n_clusters\", \"silhouette_score\"])\n",
    "    pivot_ac = pd.pivot_table(result, index=\"n_clusters\", values=\"silhouette_score\")\n",
    "\n",
    "    return result, pivot_ac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc84bf15",
   "metadata": {},
   "outputs": [],
   "source": [
    "result, pivot_ac = visualize_silhouette_layer1(data[\"review vector\"],10)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5345d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(result.n_clusters, result.silhouette_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50cff894",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 군집에서의 문장을 sample해서 봐보기 위한 코드입니다\n",
    "\n",
    "cluster_index = range(6) #클러스터 개수에 따라 range()에 숫자넣기!\n",
    "\n",
    "representative_sentence = {}\n",
    "\n",
    "for i in cluster_index:\n",
    "    sent_sample = data[\"review vector\"][data['predict']==i].sample(n=2, random_state=26)\n",
    "    representative_sentence[str(i)+'번 군집'] = sent_sample.iloc[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22fe3d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "representative_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e9aa4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['predict'] = predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e288ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster0= []\n",
    "cluster1= []\n",
    "cluster2= []\n",
    "cluster3= []\n",
    "cluster4= []\n",
    "cluster5= []\n",
    "\n",
    "for i in range(len(data[\"review vector\"])):\n",
    "    if data['predict'][i] == 0:\n",
    "        for e in df_pos18[\"review vector\"][i]:\n",
    "            cluster0.append(e)\n",
    "            \n",
    "    elif data['predict'][i] == 1:\n",
    "        for e in df_pos18[\"review vector\"][i]:\n",
    "            cluster1.append(e)\n",
    "                \n",
    "    elif data['predict'][i] == 2:\n",
    "        for e in df_pos18[\"review vector\"][i]:\n",
    "            cluster2.append(e)\n",
    "            \n",
    "    elif data['predict'][i] == 3:\n",
    "        for e in df_pos18[\"review vector\"][i]:\n",
    "            cluster3.append(e)\n",
    "                \n",
    "    elif data['predict'][i] == 4:\n",
    "        for e in df_pos18[\"review vector\"][i]:\n",
    "            cluster4.append(e)\n",
    "            \n",
    "    elif data['predict'][i] == 5:\n",
    "        for e in df_pos18[\"review vector\"][i]:\n",
    "            cluster5.append(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404c51f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#빈도분석\n",
    "korean = pd.Series(cluster3).value_counts().head(10)\n",
    "print(korean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3672ff6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "korean = pd.Series(cluster0).value_counts()\n",
    "counts0 = korean\n",
    "\n",
    "korean = pd.Series(cluster1).value_counts()\n",
    "counts1 = korean\n",
    "\n",
    "korean = pd.Series(cluster2).value_counts()\n",
    "counts2 = korean\n",
    "\n",
    "korean = pd.Series(cluster3).value_counts()\n",
    "counts3 = korean\n",
    "\n",
    "korean = pd.Series(cluster4).value_counts()\n",
    "counts4 = korean\n",
    "\n",
    "korean = pd.Series(cluster5).value_counts()\n",
    "counts5 = korean\n",
    "\n",
    "# korean = pd.Series(cluster6).value_counts()\n",
    "# counts6 = korean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5043f57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_list = [counts0,counts1,counts2,counts3,counts4,counts5]\n",
    "\n",
    "list_list = []\n",
    "for i in count_list:\n",
    "    for w in i.index:\n",
    "        list_list.append(w)\n",
    "dic = pd.DataFrame(list_list)\n",
    "dic = dic.value_counts()\n",
    "dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2463cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "246f921b",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ae488f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# N개(모든) 클러스터에 있는 토큰 제외\n",
    "\n",
    "tfidf = []\n",
    "for i in count_list:\n",
    "    imsi = []\n",
    "    for w in i.index:\n",
    "        if dic[w] == 6:\n",
    "            imsi.append(0)\n",
    "        else:\n",
    "            imsi.append(i[w]/np.exp(dic[w]))\n",
    "    tfidf.append(imsi)\n",
    "tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2174f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf0 = {\"단어\" : counts0.index, \"tfidf\" : tfidf[0]}\n",
    "tfidf1 = {\"단어\" : counts1.index, \"tfidf\" : tfidf[1]}\n",
    "tfidf2 = {\"단어\" : counts2.index, \"tfidf\" : tfidf[2]}\n",
    "tfidf3 = {\"단어\" : counts3.index, \"tfidf\" : tfidf[3]}\n",
    "tfidf4 = {\"단어\" : counts4.index, \"tfidf\" : tfidf[4]}\n",
    "tfidf5 = {\"단어\" : counts5.index, \"tfidf\" : tfidf[5]}\n",
    "# tfidf6 = {\"단어\" : counts6.index, \"tfidf\" : tfidf[6]}\n",
    "\n",
    "tfidf0 = pd.DataFrame(tfidf0)\n",
    "tfidf1 = pd.DataFrame(tfidf1)\n",
    "tfidf2 = pd.DataFrame(tfidf2)\n",
    "tfidf3 = pd.DataFrame(tfidf3)\n",
    "tfidf4 = pd.DataFrame(tfidf4)\n",
    "tfidf5 = pd.DataFrame(tfidf5)\n",
    "# tfidf6 = pd.DataFrame(tfidf6)\n",
    "\n",
    "tfidf0 = tfidf0.sort_values(by = \"tfidf\", ascending = False)\n",
    "tfidf1 = tfidf1.sort_values(by = \"tfidf\", ascending = False)\n",
    "tfidf2 = tfidf2.sort_values(by = \"tfidf\", ascending = False)\n",
    "tfidf3 = tfidf3.sort_values(by = \"tfidf\", ascending = False)\n",
    "tfidf4 = tfidf4.sort_values(by = \"tfidf\", ascending = False)\n",
    "tfidf5 = tfidf5.sort_values(by = \"tfidf\", ascending = False)\n",
    "# tfidf6 = tfidf6.sort_values(by = \"tfidf\", ascending = False)\n",
    "\n",
    "# # CSV 저장\n",
    "tfidf0.to_csv('shopping_tfidf0.csv', encoding = \"utf-8-sig\")\n",
    "tfidf1.to_csv('shopping_tfidf1.csv', encoding = \"utf-8-sig\")\n",
    "tfidf2.to_csv('shopping_tfidf2.csv', encoding = \"utf-8-sig\")\n",
    "tfidf3.to_csv('shopping_tfidf3.csv', encoding = \"utf-8-sig\")\n",
    "tfidf4.to_csv('shopping_tfidf4.csv', encoding = \"utf-8-sig\")\n",
    "tfidf5.to_csv('shopping_tfidf5.csv', encoding = \"utf-8-sig\")\n",
    "# tfidf6.to_csv('중_tfidf6.csv', encoding = \"utf-8-sig\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
